{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00be8279",
   "metadata": {},
   "source": [
    "# \"How to make your models available to the public\"\n",
    "> \"Explaining how to use Docker, Flask and Gunicorn to deploy your models online\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [mlops, docker, computer-vision, ai]\n",
    "- image: images/docker-blog/mlops-loop-dockerblog.jpg\n",
    "- hide: false\n",
    "- search_exclude: false\n",
    "- author: Suvaditya Mukherjee\n",
    "- hide_binder_badge: true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f678d1dc",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "An end-to-end Machine Learning solution is an important way to bring AI to production and make it available for mass consumption and usage. But today, most AI practitioners simply do the pre-processing, training, evaluation and tuning stages and leave the remaining part to DevOps engineers. \n",
    "\n",
    "As such, a new field of development named [MLOps](https://blogs.nvidia.com/blog/2020/09/03/what-is-mlops/) has come into the mainstream. The focus has shifted from simply training and evaluation to also bringing and integrating it to production pipelines.\n",
    "\n",
    "On an individual level as well, knowing how to bring your model to the public is an important tool to have in an AI practitioner's skill-set. \n",
    "\n",
    "In this article, we will be exploring how we can perform a small segment of the MLOps cycle in a simple and efficient manner using **Keras**, **Flask**, **Gunicorn** and **Docker**.  \n",
    "\n",
    "If you wish to skip through and go straight to the code, [click here to go to the GitHub repository](https://github.com/suvadityamuk/KerasDocker)\n",
    "\n",
    "![](../images/docker-blog/dockerblog1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc506dc6",
   "metadata": {},
   "source": [
    "## What is covered in this tutorial?\n",
    "\n",
    "1) Create a custom model using **`Keras`** and its off-the-shelf components  \n",
    "2) Prepare an inference pipeline  \n",
    "3) Develop a simple `Flask` app to **expose the model for inference**  \n",
    "4) Define a `Dockerfile` using `Gunicorn`  \n",
    "5) **Build our image**\n",
    "6) Define a simple **Github Actions workflow** to build your image every time you push it to your repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2b712b",
   "metadata": {},
   "source": [
    "## 1) Create a custom model using Keras\n",
    "\n",
    "As an example, we are going to create a simple model using the Keras Functional API and an off-the-shelf MobileNetV2 model from `keras.applications` pretrained on ImageNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e6c591",
   "metadata": {},
   "source": [
    "### Import headers\n",
    "\n",
    "We require `tensorflow`, `keras`, `Flask`, `PIL` and `os` for this tutorial. If using a virtual environment, you can use the `requirements.txt` file below to get your env prepared.\n",
    "\n",
    "- `tensorflow`: Used for matrix operations and back-end for keras\n",
    "- `keras`: Used for high-level Deep Learning model-building API and get pre-trained model\n",
    "- `Flask`: Used for building simple API for inference\n",
    "- `PIL`: Used for handling images\n",
    "- `os`: Used for setting environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78409d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from flask import Flask\n",
    "from flask import request, jsonify\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f3d2b",
   "metadata": {},
   "source": [
    "### Set options\n",
    "\n",
    "Since GPUs are a difficult resource to get a hold of, we set a Tensorflow flag to make any CUDA devices present invisible in the first place. *If you can run your container on a GPU, feel free to skip this line.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea58cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To force inference using CPU only\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b967ee96",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "\n",
    "This model is made using the Keras Functional API. We take a simple `keras.Input` which accepts color (RGB) images of any size.  \n",
    "The input is passed via the following layers:  \n",
    "- `keras.layers.Resizing` : Used to resize the image tensor to a **224x224x3** tensor\n",
    "- `keras.layers.Rescaling` : Used to rescale the image tensor values from a [0,255] range to a [0,1] range\n",
    "- `keras.applications.MobileNetV2` : Used to import the **MobileNetV2** instance from Keras (pretrained on ImageNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = keras.Input(shape=(None,None,3))\n",
    "\n",
    "x = keras.layers.Resizing(height=224, width=224, interpolation='lanczos3', crop_to_aspect_ratio=False)(image_input)\n",
    "\n",
    "x = keras.layers.Rescaling(scale=1./255, offset=0.0)(x)\n",
    "\n",
    "mobilenet = keras.applications.MobileNetV2(\n",
    "    alpha=1.0,\n",
    "    include_top=True,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=image_input,\n",
    "    classes=1000,\n",
    "    classifier_activation=\"softmax\"\n",
    ")\n",
    "\n",
    "model_output = mobilenet(x)\n",
    "\n",
    "model = keras.Model(inputs=image_input, outputs=model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc474a",
   "metadata": {},
   "source": [
    "### Requirements file\n",
    "\n",
    "`Gunicorn` is used to deploy the API on several workers together to allow lower latency at the expense of increased compute consumption. Gunicorn is used since it implements WSGI. In a production environment, a front-facing server like [**NGINX**](https://www.nginx.com/) or [**Apache Web Server**](https://httpd.apache.org/) is used to host Static web pages and load balancers with Gunicorn running behind this layer to enable functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca264113",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flask==2.0.3\n",
    "Pillow==9.2.0\n",
    "tensorflow==2.9.1\n",
    "gunicorn==20.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50ff29f",
   "metadata": {},
   "source": [
    "## 2) Prepare an inference pipeline\n",
    "\n",
    "We define a simple function which accepts a `tf.Tensor` and runs it through the model to return a final top-5 predictions dictionary result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cad462b",
   "metadata": {},
   "source": [
    "### Inference function\n",
    "\n",
    "The image, accepted as a `tf.Tensor`, is inferred using the function prepared before. The `numpy` value of the tensor is then extracted to get all the confidence scores for each class.  \n",
    "This numpy array is then passed into `keras.applications.imagenet_utils.decode_predictions` to get the top 5 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dcf012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(image: tf.Tensor):\n",
    "    y = model(image).numpy()\n",
    "    preds = keras.applications.imagenet_utils.decode_predictions(y, top=5)\n",
    "    result = {i[1] : str(i[2]) for i in preds[0]}\n",
    "    result = {k: v for k, v in sorted(result.items(), key=lambda item: item[1])}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923bad7f",
   "metadata": {},
   "source": [
    "## 3) Make a simple Flask App to expose model for inference\n",
    "\n",
    "Now, we define 2 simple endpoints at the routes `/` and `/inference`.  \n",
    "- `/` (GET) : The first endpoint acts as a health-check to make sure that the API is up and running  \n",
    "- `/inference` (POST) : The second endpoint accepts an image as a form field with the parameter name `image` and returns a dictionary with the confidence scores and the ImageNet class names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18875c2b",
   "metadata": {},
   "source": [
    "### Flask App definition\n",
    "\n",
    "`app` is the name of the WSGI callable that will be used by Gunicorn later on. To know more about what WSGI is, check the Interesting Links section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c50fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b95ba2",
   "metadata": {},
   "source": [
    "### Definition of health-check endpoint\n",
    "\n",
    "To test whether the API is up and running, we simply hit a GET request on this endpoint to get the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af07b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route(\"/\", methods=['GET'])\n",
    "def health_check():\n",
    "    result = {\n",
    "        'outcome':'endpoint working successfully'\n",
    "    }\n",
    "    return jsonify(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f419e3cb",
   "metadata": {},
   "source": [
    "### Definition of inference endpoint\n",
    "\n",
    "Here, we accept a `POST` request, extract the `image` parameter from the files sent in the request. This is stored in a file-stream format which is then passed into a `PIL.Image.open` to prepare the image. Finally, we perform some simple pre-processing to convert the `PIL` image to a `tf.Tensor` and prepare a batch of 1 image to be passed into our inference function. The result returned is then passed into `jsonify` for response preparation and execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route(\"/inference\", methods=['POST'])\n",
    "def perform_inference():\n",
    "    image = request.files['image']\n",
    "    pil_img = Image.open(image.stream)\n",
    "    tensor = keras.preprocessing.image.img_to_array(pil_img)\n",
    "    tensor = tf.expand_dims(tensor, axis=0)\n",
    "    result = inference(tensor)\n",
    "    return jsonify(result)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba395694",
   "metadata": {},
   "source": [
    "## 4) Define a Dockerfile which uses Gunicorn for deployment\n",
    "\n",
    "We are now done with defining our model and preparing it for inference using a simple Flask App. Here, we begin writing a `Dockerfile` and a `.dockerignore` to build a custom Docker Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714cffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM ubuntu:20.04\n",
    "\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "git \\\n",
    "curl \\\n",
    "ca-certificates \\\n",
    "python3 \\\n",
    "python3-pip \\\n",
    "sudo \\\n",
    "&& rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "RUN useradd -m docker_runner\n",
    "\n",
    "RUN chown -R docker_runner:docker_runner /home/docker_runner\n",
    "\n",
    "COPY --chown=docker_runner *.* /home/docker_runner/flask_app/keras-docker-trial/\n",
    "\n",
    "USER docker_runner\n",
    "\n",
    "WORKDIR /home/docker_runner/flask_app/keras-docker-trial\n",
    "\n",
    "ENV PATH=\"${PATH}:/home/docker_runner/.local/bin\"\n",
    "\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "ENTRYPOINT [\"gunicorn\", \"--bind\", \"0.0.0.0:5000\", \"--workers=4\", \"app:app\"]\n",
    "\n",
    "EXPOSE 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecb5c8b",
   "metadata": {},
   "source": [
    "### Dockerfile\n",
    "\n",
    "- The first line pulls the `ubuntu:20.04` image from Docker Hub to prepare a container with stock Ubuntu 20.04 Focal Fossa within it.  \n",
    "- The first `RUN` command downloads and installs several essential packages that we require later ahead.\n",
    "- The next `RUN` command adds a user named docker_runner and creates a home directory for the user (using the -m option)\n",
    "- The next `RUN` command changes directory ownership and assigns docker_runner as the owner of its own home directory in a recursive manner for all files and subdirectories as well (using the -R option)\n",
    "- The `COPY` command moves all the files present in the current repository where the Dockerfile is into the container's target directory\n",
    "- The `USER` command is used to change the current active user to `docker_runner`\n",
    "- The `WORKDIR` command is used to change the current active directory to `/home/docker_runner/flask_app/keras-docker-trial`\n",
    "- The `ENV` command is used to set the PATH environment variable and add our user's `/.local/bin` directory to it\n",
    "- The `RUN` command is now used to install all the requirements and not use any cached directories or their SHA hashes while doing so\n",
    "- The `ENTRYPOINT` command is used to begin the API deployment using `gunicorn`. We bind the localhost's port 5000 and start up 4 workers for this task. We specify the WSGI callable as `app` on the left side of `app:app`. If you changed the name of the Flask app in Step 3, then you should change this part as `{your_app_name}:app`\n",
    "- The `EXPOSE` command is used to make the container listen on port 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952668ec",
   "metadata": {},
   "source": [
    "### .dockerignore\n",
    "\n",
    "We just ignore the `__pycache__/` directory as it generates intermediate files from CPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d77ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "__pycache__/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912dc532",
   "metadata": {},
   "source": [
    "## 5) Build our image and deploy it to Docker Hub\n",
    "\n",
    "We now build our image and assign it a tag `keras-docker-trial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db68153",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker build . -t keras-docker-trial --file Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061cd8bc",
   "metadata": {},
   "source": [
    "## 6) Define a simple GitHub Actions workflow to build your image every time you push it to your repository\n",
    "\n",
    "Here, as an extra step, we use GitHub Actions to build our image as a test every time a Push is made to any branch or if a PR is merged in the repository. This needs to be added only if you are preparing a repository on GitHub for your model.\n",
    "\n",
    "- `name` : Assigns a name to the workflow\n",
    "- `on` : Defines the triggers for when the workflow is to be used\n",
    "- `env` : Sets environment variables\n",
    "- `jobs` : Defines the different commands and workflow actions to be run as part of the current workflow\n",
    "- `runs-on` : Defines which GitHub-hosted runner is used for execution of workflow\n",
    "- `actions/checkout@v3` : Used to check-out the code from repository\n",
    "- `Build Docker Image` : Build image from Dockerfile present in repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d70a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "name: Docker CI\n",
    "\n",
    "on:\n",
    "  push:\n",
    "  pull_request:\n",
    "    types: ['opened', 'reopened']\n",
    "\n",
    "env:\n",
    "  BUILD_CONFIGURATION: Release\n",
    "\n",
    "jobs:\n",
    "  job1:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Check-out the pushed code\n",
    "        uses: actions/checkout@v3\n",
    "\n",
    "      - name: Build Docker image\n",
    "        run: docker build . -t keras-docker-trial --file Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5bc22a",
   "metadata": {},
   "source": [
    "## Test the pipeline\n",
    "\n",
    "Above, we have defined the model and deployed it using Docker and Gunicorn. You can find some example screenshots of the deployment and its predictions via Postman API Explorer below. \n",
    "\n",
    "![](../images/docker-blog/terminal.png)\n",
    "<p align=\"center\">Terminal command</p>\n",
    "\n",
    "![](../images/docker-blog/health-check.png)\n",
    "<p align=\"center\">GET request on health-check</p>\n",
    "\n",
    "![](../images/docker-blog/inference.png)\n",
    "<p align=\"center\">GET request on inference</p>\n",
    "\n",
    "![](../images/docker-blog/goldfish.jpg)\n",
    "<p align=\"center\">The Goldfish image sent for request via Postman </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f8961d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Above, we have completed the development of a simple Keras model, its deployment via Docker and a GitHub Actions workflow for CI(Continuous Integration).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8664012",
   "metadata": {},
   "source": [
    "## Future Scope\n",
    "\n",
    "This is only a small part of what can be done as a part of a simple MLOps pipeline. CML (Continuous Machine Learning) and DVC (Data Version Control) are two important concepts that are an integral part of every self-sustaining machine learning workflow and can be explored further. Resources to do so are present in the Interesting Links section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8d399b",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1.) [Docker Hub Documentation](https://docs.docker.com/docker-hub/)  \n",
    "2.) [Keras Applications Documentation](https://keras.io/api/applications/mobilenet/#mobilenetv2-function)  \n",
    "3.) [Gunicorn Documentation](https://docs.gunicorn.org/en/stable/configure.html)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e924c779",
   "metadata": {},
   "source": [
    "## Interesting Links\n",
    "\n",
    "1.) [What is CML?](https://cml.dev/)  \n",
    "2.) [What is DVC?](https://dvc.org/doc/user-guide/what-is-dvc)  \n",
    "3.) [What is WSGI (Web Server Gateway Interface)?](https://wsgi.readthedocs.io/en/latest/what.html)  \n",
    "4.) [Detailed blog on What is MLOps?](https://neptune.ai/blog/mlops)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oss-ml",
   "language": "python",
   "name": "oss-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
