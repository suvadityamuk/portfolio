{
  
    
        "post0": {
            "title": "How to make your models available to the public",
            "content": "Introduction . An end-to-end Machine Learning solution is an important way to bring AI to production and make it available for mass consumption and usage. But today, most AI practitioners simply do the pre-processing, training, evaluation and tuning stages and leave the remaining part to DevOps engineers. . As such, a new field of development named MLOps has come into the mainstream. The focus has shifted from simply training and evaluation to also bringing and integrating it to production pipelines. . On an individual level as well, knowing how to bring your model to the public is an important tool to have in an AI practitioner&#39;s skill-set. . In this article, we will be exploring how we can perform a small segment of the MLOps cycle in a simple and efficient manner using Keras, Flask, Gunicorn and Docker. . If you wish to skip through and go straight to the code, click here to go to the GitHub repository . . What is covered in this tutorial? . 1) Create a custom model using Keras and its off-the-shelf components 2) Prepare an inference pipeline 3) Develop a simple Flask app to expose the model for inference 4) Define a Dockerfile using Gunicorn 5) Build our image 6) Define a simple Github Actions workflow to build your image every time you push it to your repository . 1) Create a custom model using Keras . As an example, we are going to create a simple model using the Keras Functional API and an off-the-shelf MobileNetV2 model from keras.applications pretrained on ImageNet. . Import headers . We require tensorflow, keras, Flask, PIL and os for this tutorial. If using a virtual environment, you can use the requirements.txt file below to get your env prepared. . tensorflow: Used for matrix operations and back-end for keras | keras: Used for high-level Deep Learning model-building API and get pre-trained model | Flask: Used for building simple API for inference | PIL: Used for handling images | os: Used for setting environment variables | . import tensorflow as tf from tensorflow import keras from flask import Flask from flask import request, jsonify from PIL import Image import os . Set options . Since GPUs are a difficult resource to get a hold of, we set a Tensorflow flag to make any CUDA devices present invisible in the first place. If you can run your container on a GPU, feel free to skip this line. . os.environ[&#39;CUDA_VISIBLE_DEVICES&#39;] = &#39;-1&#39; . Model definition . This model is made using the Keras Functional API. We take a simple keras.Input which accepts color (RGB) images of any size. The input is passed via the following layers: . keras.layers.Resizing : Used to resize the image tensor to a 224x224x3 tensor | keras.layers.Rescaling : Used to rescale the image tensor values from a [0,255] range to a [0,1] range | keras.applications.MobileNetV2 : Used to import the MobileNetV2 instance from Keras (pretrained on ImageNet) | . image_input = keras.Input(shape=(None,None,3)) x = keras.layers.Resizing(height=224, width=224, interpolation=&#39;lanczos3&#39;, crop_to_aspect_ratio=False)(image_input) x = keras.layers.Rescaling(scale=1./255, offset=0.0)(x) mobilenet = keras.applications.MobileNetV2( alpha=1.0, include_top=True, weights=&quot;imagenet&quot;, input_tensor=image_input, classes=1000, classifier_activation=&quot;softmax&quot; ) model_output = mobilenet(x) model = keras.Model(inputs=image_input, outputs=model_output) . Requirements file . Gunicorn is used to deploy the API on several workers together to allow lower latency at the expense of increased compute consumption. Gunicorn is used since it implements WSGI. In a production environment, a front-facing server like NGINX or Apache Web Server is used to host Static web pages and load balancers with Gunicorn running behind this layer to enable functionality. . Flask==2.0.3 Pillow==9.2.0 tensorflow==2.9.1 gunicorn==20.1.0 . 2) Prepare an inference pipeline . We define a simple function which accepts a tf.Tensor and runs it through the model to return a final top-5 predictions dictionary result. . Inference function . The image, accepted as a tf.Tensor, is inferred using the function prepared before. The numpy value of the tensor is then extracted to get all the confidence scores for each class. This numpy array is then passed into keras.applications.imagenet_utils.decode_predictions to get the top 5 predictions. . def inference(image: tf.Tensor): y = model(image).numpy() preds = keras.applications.imagenet_utils.decode_predictions(y, top=5) result = {i[1] : str(i[2]) for i in preds[0]} result = {k: v for k, v in sorted(result.items(), key=lambda item: item[1])} return result . 3) Make a simple Flask App to expose model for inference . Now, we define 2 simple endpoints at the routes / and /inference. . / (GET) : The first endpoint acts as a health-check to make sure that the API is up and running | /inference (POST) : The second endpoint accepts an image as a form field with the parameter name image and returns a dictionary with the confidence scores and the ImageNet class names | . Flask App definition . app is the name of the WSGI callable that will be used by Gunicorn later on. To know more about what WSGI is, check the Interesting Links section below. . app = Flask(__name__) . Definition of health-check endpoint . To test whether the API is up and running, we simply hit a GET request on this endpoint to get the expected output. . @app.route(&quot;/&quot;, methods=[&#39;GET&#39;]) def health_check(): result = { &#39;outcome&#39;:&#39;endpoint working successfully&#39; } return jsonify(result) . Definition of inference endpoint . Here, we accept a POST request, extract the image parameter from the files sent in the request. This is stored in a file-stream format which is then passed into a PIL.Image.open to prepare the image. Finally, we perform some simple pre-processing to convert the PIL image to a tf.Tensor and prepare a batch of 1 image to be passed into our inference function. The result returned is then passed into jsonify for response preparation and execution . @app.route(&quot;/inference&quot;, methods=[&#39;POST&#39;]) def perform_inference(): image = request.files[&#39;image&#39;] pil_img = Image.open(image.stream) tensor = keras.preprocessing.image.img_to_array(pil_img) tensor = tf.expand_dims(tensor, axis=0) result = inference(tensor) return jsonify(result) . 4) Define a Dockerfile which uses Gunicorn for deployment . We are now done with defining our model and preparing it for inference using a simple Flask App. Here, we begin writing a Dockerfile and a .dockerignore to build a custom Docker Image . FROM ubuntu:20.04 RUN apt-get update &amp;&amp; apt-get install -y git curl ca-certificates python3 python3-pip sudo &amp;&amp; rm -rf /var/lib/apt/lists/* RUN useradd -m docker_runner RUN chown -R docker_runner:docker_runner /home/docker_runner COPY --chown=docker_runner *.* /home/docker_runner/flask_app/keras-docker-trial/ USER docker_runner WORKDIR /home/docker_runner/flask_app/keras-docker-trial ENV PATH=&quot;${PATH}:/home/docker_runner/.local/bin&quot; RUN pip install --no-cache-dir -r requirements.txt ENTRYPOINT [&quot;gunicorn&quot;, &quot;--bind&quot;, &quot;0.0.0.0:5000&quot;, &quot;--workers=4&quot;, &quot;app:app&quot;] EXPOSE 5000 . Dockerfile . The first line pulls the ubuntu:20.04 image from Docker Hub to prepare a container with stock Ubuntu 20.04 Focal Fossa within it. | The first RUN command downloads and installs several essential packages that we require later ahead. | The next RUN command adds a user named docker_runner and creates a home directory for the user (using the -m option) | The next RUN command changes directory ownership and assigns docker_runner as the owner of its own home directory in a recursive manner for all files and subdirectories as well (using the -R option) | The COPY command moves all the files present in the current repository where the Dockerfile is into the container&#39;s target directory | The USER command is used to change the current active user to docker_runner | The WORKDIR command is used to change the current active directory to /home/docker_runner/flask_app/keras-docker-trial | The ENV command is used to set the PATH environment variable and add our user&#39;s /.local/bin directory to it | The RUN command is now used to install all the requirements and not use any cached directories or their SHA hashes while doing so | The ENTRYPOINT command is used to begin the API deployment using gunicorn. We bind the localhost&#39;s port 5000 and start up 4 workers for this task. We specify the WSGI callable as app on the left side of app:app. If you changed the name of the Flask app in Step 3, then you should change this part as {your_app_name}:app | The EXPOSE command is used to make the container listen on port 5000 | . .dockerignore . We just ignore the __pycache__/ directory as it generates intermediate files from CPython . __pycache__/ . 5) Build our image and deploy it to Docker Hub . We now build our image and assign it a tag keras-docker-trial. . docker build . -t keras-docker-trial --file Dockerfile . 6) Define a simple GitHub Actions workflow to build your image every time you push it to your repository . Here, as an extra step, we use GitHub Actions to build our image as a test every time a Push is made to any branch or if a PR is merged in the repository. This needs to be added only if you are preparing a repository on GitHub for your model. . name : Assigns a name to the workflow | on : Defines the triggers for when the workflow is to be used | env : Sets environment variables | jobs : Defines the different commands and workflow actions to be run as part of the current workflow | runs-on : Defines which GitHub-hosted runner is used for execution of workflow | actions/checkout@v3 : Used to check-out the code from repository | Build Docker Image : Build image from Dockerfile present in repository | . name: Docker CI on: push: pull_request: types: [&#39;opened&#39;, &#39;reopened&#39;] env: BUILD_CONFIGURATION: Release jobs: job1: runs-on: ubuntu-latest steps: - name: Check-out the pushed code uses: actions/checkout@v3 - name: Build Docker image run: docker build . -t keras-docker-trial --file Dockerfile . Test the pipeline . Above, we have defined the model and deployed it using Docker and Gunicorn. You can find some example screenshots of the deployment and its predictions via Postman API Explorer below. . . Terminal command . GET request on health-check . GET request on inference . The Goldfish image sent for request via Postman . Conclusion . Above, we have completed the development of a simple Keras model, its deployment via Docker and a GitHub Actions workflow for CI(Continuous Integration). . Future Scope . This is only a small part of what can be done as a part of a simple MLOps pipeline. CML (Continuous Machine Learning) and DVC (Data Version Control) are two important concepts that are an integral part of every self-sustaining machine learning workflow and can be explored further. Resources to do so are present in the Interesting Links section. . References . 1.) Docker Hub Documentation 2.) Keras Applications Documentation 3.) Gunicorn Documentation . Interesting Links . 1.) What is CML? 2.) What is DVC? 3.) What is WSGI (Web Server Gateway Interface)? 4.) Detailed blog on What is MLOps? .",
            "url": "https://suvadityamuk.github.io/portfolio/mlops/docker/computer-vision/ai/2022/09/05/How-To-Make-Your-Models-Available-To-The-Public.html",
            "relUrl": "/mlops/docker/computer-vision/ai/2022/09/05/How-To-Make-Your-Models-Available-To-The-Public.html",
            "date": " • Sep 5, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "The Annotated ResNet-50",
            "content": "Introduction . The ResNet architecture is considered to be among the most popular Convolutional Neural Network architectures around. Introduced by Microsoft Research in 2015, Residual Networks (ResNet in short) broke several records when it was first introduced in this paper by He. et. al . Why ResNet? . The requirement for a model like ResNet arose due to a number of pitfalls in modern networks at the time. . Difficulty in training deep neural networks: As the number of layers in a model increases, the number of parameters in the model increases exponentially. For each Convolutional layer, a total of $((height_{kernel} cdot width_{kernel} cdot filters_{input}) + 1) cdot filters_{output}$ gets added to the bill. To put it into context, a simple 7x7 kernel Convolution layer from 3 channels to 32 channels adds 4736 parameters. An increase in the number of layers in the interest of experimentation leads to an equal increase in complexity for training the model. Training then requires greater computational power and memory. . | More expressive, less different: A neural network is often considered to be a function approximator. It has the ability to model functions given input, target and a comparison between the function output and target. Adding multiple layers into a network makes it more capable to model complex functions. But results published in the paper stated that a 20-layer plain neural network performs considerably better than a 56-layer plain neural network as can be seen in the below graph. (He et al., 2015) . Adding layers can be seen as an expansion of the function space. For example, multiple layers added together can be seen as a function $F$. This function $F$ can be expressed as a representation of a function space $F`$ that it can reach/model. Having your desirable function in $F&#39;$ would be a lucky chance, but more often than not, it is not the case. Adding layers here allows us to expand and change around the function space $F&#39;$, allowing us to cover a larger space in the larger parent function space consisting of all possible functions in the conceivable universe. But this method has an inherent pitfall. As the function space becomes larger, there is no guarantee that we get closer to our target function. In fact, there is a good chance that in the experimental phase, you move away from the function space that may have the function you actually need. . Did the jargon confuse you? Let&#39;s take an analogy of a needle and a haystack. Let the needle be the perfect weights of the neural network, or as explained before, a function. Let the haystack be all the possible functions that can be made. One starts from a single search area and tries to zero into the needle from there. Adding layers is equivalent to moving your search area and making it bigger. But that comes with the risk of moving away from the place where the needle actually is as well as making our search more time-consuming and difficult. Larger the haystack, more difficult it is to find the perfect needle. What is the solution, then? . Quite simple and elegant, actually. Nest your function spaces. This is done for a few simple reasons. The most important one being the fact that it allows you to ensure that while the model adds layers to increase the size of the function space, you don&#39;t end up degrading the model. This gives the guarantee that while our model can do better with more layers, it will not do any worse. Coming back to our haystack analogy, this is equivalent to making our search space larger, but making sure that we do not move away from our current search space. . (Zhang et al., 2021) . | Vanishing/Exploding Gradient: This is one of the most common problems plaguing the training of larger/deep neural networks and is a result of oversight in terms of numerical stability of the network&#39;s parameters. During backpropagation, as we keep moving from the deep to the shallow layers, the chain rule of differentiation makes us multiply the gradients. Often, these gradients are small, to the order of $10^{-5}$ or more. According to some simple math, as these small numbers keep getting multiplied with each other, they keep becoming infinitesimally smaller, making almost negligible changes to the weights. On the other end of the spectrum, there are cases when the gradient reaches orders upto $10^{4}$ and more. As these large gradients multiply with each other, the values tend to move towards infinity. Allowing such a large range of values to be in the numerical domain for weights makes convergence difficult to achieve. This problem is popularly known as the Vanishing/Exploding gradient problem. ResNet, due to its architecture, does not allow these problems to occur at all. How so? The skip connections (described ahead) do not allow it as they act as gradient super-highways, allowing it to flow without being altered by a large magnitude. . | What are Skip Connections? . The ResNet paper popularized the approach of using Skip Connections. If you recall, the approach to solving our function space problems was to nest them. In terms of applying it to our use-case, it was the introduction of a simple addition of the identity function to the output. In mathematical terms, it would mean $y = x + F(x)$ where y is the final output of the layer. . (He et al., 2015) . In terms of architecture, if any layer ends up damaging the performance of the model in a plain network, it gets skipped due to the presence of the skip-connections . Architecture . The ResNet-50 architecture can be broken down into 6 parts . Input Pre-processing | Cfg[0] blocks | Cfg[1] blocks | Cfg[2] blocks | Cfg[3] blocks | Fully-connected layer | Different versions of the ResNet architecture use a varying number of Cfg blocks at different levels, as mentioned in the figure above. A detailed, informative listing can be found below . . Show me the code! . The best way to understand the concept is through some code. The implementation below is done in Keras, uses the standard ResNet-50 architecture (ResNet has several versions, differing in the depth of the network). We will train the model on the famous Stanford Dogs dataset by Stanford AI . Import headers . !pip install -q tfds import tensorflow as tf from tensorflow import keras import tensorflow_datasets as tfds import os import PIL import pathlib import PIL.Image import warnings warnings.filterwarnings(&quot;ignore&quot;) from datetime import datetime . . Dataset download and pre-processing . We download the Stanford Dogs dataset using Tensorflow Datasets (stable) and split it into a training, validation and test set. Along with the images and labels, we also get some meta-data which gives us more information about the dataset. That is stored in ds_info and printed in a human-readable manner. We also make use of tfds.show_examples() to print some random example images and labels from the dataset. We run tfds.benchmark() to perform a benchmarking test on the iterator provided by tf.data.Dataset We perform the following best-practice steps on the tf.data.Dataset object to make it efficient: . batch(BATCH_SIZE) : Allows us to prepare mini-batches within the dataset. Note that the batching operation requires all images to be of the same size and have the same number of channels | map(format_image) : Cast the image into a tf.float32 Tensor, normalize all values in the range $[0,1]$, resize the image from its original shape to the model-input shape of $(224, 224, 3)$ using the lanczos3 kernel method | prefetch(BUFFER_SIZE) : Pre-fetch brings in the next batch of the dataset during training into memory while the current batch is being processed, reducing the I/O time but requiring more memory in the GPU | cache() : Caches the first batch of the iterator to reduce load-times, similar to prefetch with the difference simply being that cache will load the files but not push into GPU memory | . (train_ds, valid_ds, test_ds), ds_info = tfds.load( &#39;stanford_dogs&#39;, split=[&#39;train&#39;, &#39;test[0%:10%]&#39;, &#39;test[10%:]&#39;], shuffle_files=True, with_info=True, as_supervised=True ) print(&quot;Dataset info: n&quot;) print(f&#39;Name: {ds_info.name} n&#39;) print(f&#39;Number of training samples : {ds_info.splits[&quot;train&quot;].num_examples} n&#39;) print(f&#39;Number of training samples : {ds_info.splits[&quot;test&quot;].num_examples} n&#39;) print(f&#39;Description : {ds_info.description}&#39;) tfds.show_examples(train_ds, ds_info) CLASS_TYPES = ds_info.features[&#39;label&#39;].num_classes BATCH_SIZE = 4 print(&#39;Benchmark results&#39;) tfds.benchmark(train_ds) def format_image(image, label): image = tf.cast(image, tf.float32) image = image / 255.0 image = tf.image.resize_with_pad(image, 224, 224, method=&#39;lanczos3&#39;, antialias=True) return image, label def prepare_ds(ds): ds = ds.map(format_image) ds = ds.batch(BATCH_SIZE) ds = ds.prefetch(tf.data.AUTOTUNE) ds = ds.cache() return ds train_ds = prepare_ds(train_ds) valid_ds = prepare_ds(valid_ds) test_ds = prepare_ds(test_ds) . . Downloading and preparing dataset 778.12 MiB (download: 778.12 MiB, generated: Unknown size, total: 778.12 MiB) to /root/tensorflow_datasets/stanford_dogs/0.2.0... Dataset stanford_dogs downloaded and prepared to /root/tensorflow_datasets/stanford_dogs/0.2.0. Subsequent calls will reuse this data. Dataset info: Name: stanford_dogs Number of training samples : 12000 Number of training samples : 8580 Description : The Stanford Dogs dataset contains images of 120 breeds of dogs from around the world. This dataset has been built using images and annotation from ImageNet for the task of fine-grained image categorization. There are 20,580 images, out of which 12,000 are used for training and 8580 for testing. Class labels and bounding box annotations are provided for all the 12,000 images. . Benchmark results ************ Summary ************ Examples/sec (First included) 787.00 ex/sec (total: 12000 ex, 15.25 sec) Examples/sec (First only) 10.34 ex/sec (total: 1 ex, 0.10 sec) Examples/sec (First excluded) 791.95 ex/sec (total: 11999 ex, 15.15 sec) . Augmentation . We perform some data augmentation to allow our model to be more robust. A RandomFlip, RandomRotation and RandomContrast is used to make the image set more varied. The parameters to the functions are probabilities, i.e. the chance that an image will undergo the selected transformation . imageAug = keras.Sequential([ keras.layers.RandomFlip(&quot;horizontal_and_vertical&quot;), keras.layers.RandomRotation(0.2), keras.layers.RandomContrast(0.2) ]) . . Cfg0 Block . This block contains 1 Conv Layer and 2 Identity Layers. For helping numerical stability, we specify a kernel constraint which makes sure that all weights are normalized at constant intervals. Between 2 subsequent layers, we also include a BatchNormalization layer. The code has been written in an explicit way deliberately to help readers understand what design choices have been made at each stage . Input Shape : $(56, 56, 64)$ | Output Shape : $(56, 56, 256)$ | . cfg0_conv_input = keras.Input(shape=(56,56,64), name=&#39;cfg0_conv&#39;) x = keras.layers.Conv2D(64, kernel_size=1, strides=1, activation=&#39;relu&#39;, padding=&#39;valid&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (cfg0_conv_input) x = keras.layers.BatchNormalization()(x) x = keras.layers.Conv2D(64, kernel_size=3, strides=1, activation=&#39;relu&#39;, padding=&#39;same&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (x) x = keras.layers.BatchNormalization()(x) x = keras.layers.Conv2D(256, kernel_size=1, strides=1, padding=&#39;valid&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (x) x = keras.layers.BatchNormalization()(x) cfg0_conv_input_transform = keras.layers.Conv2D(256, kernel_size=1, strides=1, activation=&#39;relu&#39;, padding=&#39;same&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (cfg0_conv_input) cfg0_conv_input_op = keras.layers.BatchNormalization()(cfg0_conv_input_transform) x = keras.layers.Add()([x, cfg0_conv_input_op]) cfg0_conv_output = keras.layers.ReLU()(x) cfg0_conv = keras.Model(inputs=cfg0_conv_input, outputs=cfg0_conv_output, name=&#39;cfg0_conv&#39;) cfg0_identity_input = keras.Input(shape=(56, 56, 256), name=&#39;cfg0_identity&#39;) x = keras.layers.Conv2D(64, kernel_size=1, strides=1, padding=&#39;valid&#39;, activation=&#39;relu&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (cfg0_identity_input) x = keras.layers.BatchNormalization()(x) x = keras.layers.Conv2D(64, kernel_size=3, strides=1, padding=&#39;same&#39;, activation=&#39;relu&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (x) x = keras.layers.BatchNormalization()(x) x = keras.layers.Conv2D(256, kernel_size=1, strides=1, padding=&#39;valid&#39;, activation=&#39;relu&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (x) x = keras.layers.BatchNormalization()(x) x = keras.layers.Add()([x, cfg0_identity_input]) cfg0_identity_output = keras.layers.ReLU()(x) cfg0_identity = keras.Model(inputs=cfg0_identity_input, outputs=cfg0_identity_output, name=&#39;cfg0_identity_p1&#39;) cfg0_input = keras.Input(shape=(56, 56, 64), name=&#39;cfg0&#39;) x = cfg0_conv(cfg0_input) x = cfg0_identity(x) cfg0_output = cfg0_identity(x) cfg0 = keras.Model(inputs=cfg0_input, outputs=cfg0_output, name=&#39;cfg0_block&#39;) . . Cfg1 Block . This block contains 1 Conv Layer and 2 Identity Layers. This is similar to the Cfg0 blocks, with the difference mainly being in the number of out_channels in the Conv and Identity layers being more. . Input Shape : $(56, 56, 256)$ | Output Shape : $(28, 28, 512)$ | . cfg1_conv_input = keras.Input(shape=(56, 56, 256), name=&#39;cfg1_conv&#39;) x = keras.layers.Conv2D(128, kernel_size=1, strides=2, activation=&#39;relu&#39;, padding=&#39;valid&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (cfg1_conv_input) x = keras.layers.BatchNormalization()(x) x = keras.layers.Conv2D(128, kernel_size=3, strides=1, activation=&#39;relu&#39;, padding=&#39;same&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (x) x = keras.layers.BatchNormalization()(x) x = keras.layers.Conv2D(512, kernel_size=1, strides=1, padding=&#39;valid&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (x) x = keras.layers.BatchNormalization()(x) cfg1_conv_input_transform = keras.layers.Conv2D(512, kernel_size=1, strides=2, activation=&#39;relu&#39;, padding=&#39;same&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (cfg1_conv_input) cfg1_conv_input_output = keras.layers.BatchNormalization()(cfg1_conv_input_transform) x = keras.layers.Add()([x, cfg1_conv_input_output]) cfg1_conv_output = keras.layers.ReLU()(x) cfg1_conv = keras.Model(inputs=cfg1_conv_input, outputs=cfg1_conv_output, name=&#39;cfg1_conv&#39;) cfg1_identity_input = keras.Input(shape=(28, 28, 512), name=&#39;cfg1_identity&#39;) x = keras.layers.Conv2D(128, kernel_size=1, strides=1, padding=&#39;valid&#39;, activation=&#39;relu&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (cfg1_identity_input) x = keras.layers.BatchNormalization()(x) x = keras.layers.Conv2D(128, kernel_size=3, strides=1, padding=&#39;same&#39;, activation=&#39;relu&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (x) x = keras.layers.BatchNormalization()(x) x = keras.layers.Conv2D(512, kernel_size=1, strides=1, padding=&#39;valid&#39;, activation=&#39;relu&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (x) x = keras.layers.BatchNormalization()(x) x = keras.layers.Add()([x, cfg1_identity_input]) cfg1_identity_output = keras.layers.ReLU()(x) cfg1_identity = keras.Model(inputs=cfg1_identity_input, outputs=cfg1_identity_output, name=&#39;cfg1_identity_p1&#39;) cfg1_input = keras.Input(shape=(56, 56, 256), name=&#39;cfg1&#39;) x = cfg1_conv(cfg1_input) x = cfg1_identity(x) x = cfg1_identity(x) cfg1_output = cfg1_identity(x) cfg1 = keras.Model(inputs=cfg1_input, outputs=cfg1_output, name=&#39;cfg1_block&#39;) . . Cfg2 Block . This block contains 1 Conv layer and 5 Identity layers. This is one of the more important blocks for ResNet as most versions of the model differ in this block-space. . Input Shape : $(28, 28, 512)$ | Output Shape : $(14, 14, 1024)$ | . cfg2_conv_input = keras.Input(shape=(28, 28, 512), name=&#39;cfg2_conv&#39;) x = keras.layers.Conv2D(256, kernel_size=1, strides=2, activation=&#39;relu&#39;, padding=&#39;valid&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (cfg2_conv_input) x = keras.layers.BatchNormalization()(x) x = keras.layers.Conv2D(256, kernel_size=3, strides=1, activation=&#39;relu&#39;, padding=&#39;same&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (x) x = keras.layers.BatchNormalization()(x) x = keras.layers.Conv2D(1024, kernel_size=1, strides=1, padding=&#39;valid&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (x) x = keras.layers.BatchNormalization()(x) cfg2_conv_input_transform = keras.layers.Conv2D(1024, kernel_size=1, strides=2, activation=&#39;relu&#39;, padding=&#39;same&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (cfg2_conv_input) cfg2_conv_input_output = keras.layers.BatchNormalization()(cfg2_conv_input_transform) x = keras.layers.Add()([x, cfg2_conv_input_output]) cfg2_conv_output = keras.layers.ReLU()(x) cfg2_conv = keras.Model(inputs=cfg2_conv_input, outputs=cfg2_conv_output, name=&#39;cfg2_conv&#39;) cfg2_identity_input = keras.Input(shape=(14, 14, 1024), name=&#39;cfg2_identity&#39;) x = keras.layers.Conv2D(256, kernel_size=1, strides=1, padding=&#39;valid&#39;, activation=&#39;relu&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (cfg2_identity_input) x = keras.layers.BatchNormalization()(x) x = keras.layers.Conv2D(256, kernel_size=3, strides=1, padding=&#39;same&#39;, activation=&#39;relu&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (x) x = keras.layers.BatchNormalization()(x) x = keras.layers.Conv2D(1024, kernel_size=1, strides=1, padding=&#39;valid&#39;, activation=&#39;relu&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (x) x = keras.layers.BatchNormalization()(x) x = keras.layers.Add()([x, cfg2_identity_input]) cfg2_identity_output = keras.layers.ReLU()(x) cfg2_identity = keras.Model(inputs=cfg2_identity_input, outputs=cfg2_identity_output, name=&#39;cfg2_identity_p1&#39;) cfg2_input = keras.Input(shape=(28, 28, 512), name=&#39;cfg2&#39;) x = cfg2_conv(cfg2_input) x = cfg2_identity(x) x = cfg2_identity(x) x = cfg2_identity(x) x = cfg2_identity(x) cfg2_output = cfg2_identity(x) cfg2 = keras.Model(inputs=cfg2_input, outputs=cfg2_output, name=&#39;cfg2_block&#39;) . . Cfg3 Block . This block contains 1 Conv Layer and 2 Identity Layers. This is the last set of Convolutional Layer blocks present in the network. . Input Shape : $(14, 14, 1024)$ | Output Shape : $(7, 7, 2048)$ | . cfg3_conv_input = keras.Input(shape=(14, 14, 1024), name=&#39;cfg3_conv&#39;) x = keras.layers.Conv2D(512, kernel_size=1, strides=2, activation=&#39;relu&#39;, padding=&#39;valid&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (cfg3_conv_input) x = keras.layers.BatchNormalization()(x) x = keras.layers.Conv2D(512, kernel_size=3, strides=1, activation=&#39;relu&#39;, padding=&#39;same&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (x) x = keras.layers.BatchNormalization()(x) x = keras.layers.Conv2D(2048, kernel_size=1, strides=1, padding=&#39;valid&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (x) x = keras.layers.BatchNormalization()(x) cfg3_conv_input_transform = keras.layers.Conv2D(2048, kernel_size=1, strides=2, activation=&#39;relu&#39;, padding=&#39;same&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (cfg3_conv_input) cfg3_conv_input_output = keras.layers.BatchNormalization()(cfg3_conv_input_transform) x = keras.layers.Add()([x, cfg3_conv_input_output]) cfg3_conv_output = keras.layers.ReLU()(x) cfg3_conv = keras.Model(inputs=cfg3_conv_input, outputs=cfg3_conv_output, name=&#39;cfg3_conv&#39;) cfg3_identity_input = keras.Input(shape=(7, 7, 2048), name=&#39;cfg3_identity&#39;) x = keras.layers.Conv2D(512, kernel_size=1, strides=1, padding=&#39;valid&#39;, activation=&#39;relu&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (cfg3_identity_input) x = keras.layers.BatchNormalization()(x) x = keras.layers.Conv2D(512, kernel_size=3, strides=1, padding=&#39;same&#39;, activation=&#39;relu&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (x) x = keras.layers.BatchNormalization()(x) x = keras.layers.Conv2D(2048, kernel_size=1, strides=1, padding=&#39;valid&#39;, activation=&#39;relu&#39;, kernel_constraint=keras.constraints.max_norm(2.)) (x) x = keras.layers.BatchNormalization()(x) x = keras.layers.Add()([x, cfg3_identity_input]) cfg3_identity_output = keras.layers.ReLU()(x) cfg3_identity = keras.Model(inputs=cfg3_identity_input, outputs=cfg3_identity_output, name=&#39;cfg3_identity_p1&#39;) cfg3_input = keras.Input(shape=(14, 14, 1024), name=&#39;cfg3&#39;) x = cfg3_conv(cfg3_input) x = cfg3_identity(x) cfg3_output = cfg3_identity(x) cfg3 = keras.Model(inputs=cfg3_input, outputs=cfg3_output, name=&#39;cfg3_block&#39;) . . Classifier Block . This block contains an AveragePooling Layer, a Dropout Layer and a Flatten layer. At this block, the feature map is finally flattened and pushed into a Fully Connected Layer which is then used for producing predictions. A Softmax activation is applied to generate logits/probabilities. . Input Shape : $(7, 7, 2048)$ | Output Shape : $( 1,$ CLASS_TYPES $)$ | . classifier_input = keras.Input(shape=(7, 7, 2048), name=&#39;classifier&#39;) x = keras.layers.AveragePooling2D(pool_size=2, padding=&#39;same&#39;)(classifier_input) x = keras.layers.Dropout(0.2)(x) x = keras.layers.Flatten()(x) classifier_output = keras.layers.Dense(CLASS_TYPES, activation=&#39;softmax&#39;, kernel_constraint=keras.constraints.max_norm(2.))(x) classifier = keras.Model(inputs=classifier_input, outputs=classifier_output, name=&#39;classifier&#39;) . . Build ResNet Model . Now we take all the blocks and join them together to create the final ResNet Model. In our entire process, we have used the Keras Functional API, which is a best-practice for Tensorflow We also perform some visualizations, namely model.summary() to print out the structure of the model&#39;s layers and keras.utils.plot_model() to plot the visualized Directed Acyclic Graph of the model that will be used by Tensorflow in the backend to streamline execution . def build_resnet_model(): resnet_input = keras.Input(shape=(224, 224, 3), name=&#39;input&#39;) x = imageAug(resnet_input) x = keras.layers.Conv2D(64, kernel_size=7, activation=&#39;relu&#39;, padding=&#39;same&#39;, strides=2, kernel_constraint=keras.constraints.max_norm(2.))(x) conv1_output = keras.layers.MaxPooling2D(pool_size=3, padding=&#39;same&#39;, strides=2) (x) x = cfg0(conv1_output) x = cfg1(x) x = cfg2(x) x = cfg3(x) model_output = classifier(x) resnet_model = keras.Model(inputs=resnet_input, outputs=model_output, name=&#39;resnet50&#39;) print(resnet_model.summary()) resnet_model.compile( optimizer=keras.optimizers.Adam(learning_rate=0.0005), loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[&#39;accuracy&#39;], ) return resnet_model model = build_resnet_model() keras.utils.plot_model(model, show_shapes=True, rankdir=&#39;TB&#39;, show_layer_activations=True, expand_nested=True) . . Model: &#34;resnet50&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input (InputLayer) [(None, 224, 224, 3)] 0 sequential (Sequential) (None, 224, 224, 3) 0 conv2d_28 (Conv2D) (None, 112, 112, 64) 9472 max_pooling2d (MaxPooling2D (None, 56, 56, 64) 0 ) cfg0_block (Functional) (None, 56, 56, 256) 148480 cfg1_block (Functional) (None, 28, 28, 512) 665600 cfg2_block (Functional) (None, 14, 14, 1024) 2641920 cfg3_block (Functional) (None, 7, 7, 2048) 10526720 classifier (Functional) (None, 120) 3932280 ================================================================= Total params: 17,924,472 Trainable params: 17,893,752 Non-trainable params: 30,720 _________________________________________________________________ None . Defining Callbacks . In model.fit(), we can define callbacks for the model that are invoked during training at pre-determined intervals. We define a Model Checkpoint callback that creates a snapshot of the model at the completion of each epoch. . callbacks_list = [ keras.callbacks.ModelCheckpoint( filepath=&#39;resnet50_model/checkpoint_{epoch:02d}.hdf5&#39;, monitor=&#39;val_loss&#39;, verbose=0, save_best_only=True, mode=&#39;auto&#39;, save_freq=&#39;epoch&#39;, options=None, initial_value_threshold=None ) ] history = model.fit( x=train_ds, validation_data=valid_ds, callbacks=callbacks_list, epochs=20 ) . ## If using Google Colaboratory, one can upload checkpoints onto Google Drive and use it directly. from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) model = keras.models.load_model(&#39;/content/gdrive/My Drive/checkpoint_18.hdf5&#39;) ## If using local Jupyter Notebooks, one can use checkpoints from local drives itself. model = keras.models.load_model(&#39;./resnet50_model/checkpoint_18.hdf5&#39;) . Get model history . We print the model history to get more information about the training process . print(history) . Predicting results . We take the trained model and use it to perform predictions on the test set as well as calculate several metrics like Loss and Accuracy . results = model.evaluate(test_ds) print(f&quot;Results : {results}&quot;) . Conclusion . Above, we have visited the Residual Network architecture, gone over its salient features, implemented a ResNet-50 model from scratch and trained it to get inferences on the Stanford Dogs dataset. As a model, ResNet brought about a revolution in the field of Computer Vision and Deep Learning simultaneously. It went on to win the ImageNet Large Scale Visual Recognition Challenge of 2015 and COCO Competition. But it was only a stepping stone to many interesting variations which yielded better results. Check the Interesting Links section below to find some great blogs and research papers for the same. . References . He, K., Zhang, X., Ren, S., &amp; Sun, J. (2015). Deep Residual Learning for Image Recognition. | Zhang, A., Lipton, Z. C., Li, M., &amp; Smola, A. J. (2021). Dive into Deep Learning. ArXiv Preprint ArXiv:2106.11342. | . Interesting Links . An overview of ResNets and its variants | Paper on Multi-scale ensemble of ResNet variants | Training a ResNet-50 on a Cloud TPU |",
            "url": "https://suvadityamuk.github.io/portfolio/scratch-models/computer-vision/ai/2022/08/05/The-Annotated-ResNet.html",
            "relUrl": "/scratch-models/computer-vision/ai/2022/08/05/The-Annotated-ResNet.html",
            "date": " • Aug 5, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Introduction . Hello! My name is Suvaditya Mukherjee. I am a student of the Bachelors in Artificial Intelligence Programme at NMIMS MPSTME, Mumbai. . I have played around and dabbled in many different disciplines of Computer Science in a quest to find out what excites me the most, and I have realized that Deep Learning and its applications in solving Real-world problems is the best way I see myself utilizing my skills for bettering the community. . I am a former Lead of the Google Developer Student Club at NMIMS MPSTME Mumbai and the former Co-Head of the Technical Software department at the International Society of Automation, NMIMS MPSTME Mumbai. . I have formerly interned for 2 months at Mosaic Wellness Pvt. Ltd. as a Software Engineer Intern . Technology Experience . Deep Learning : Tensorflow/Keras, PyTorch, Weights&amp;Biases, Optuna | Machine Learning : Scikit-Learn, Pandas, Matplotlib, NumPy, CuPy, Seaborn, OpenCV | Cloud Technologies : Amazon Web Services, Google Cloud, Heroku (Salesforce) | Mobile App Development : Java, Flutter(Dart) | Web Development : React, Node.js, Bootstrap, HTML/CSS | Languages : Python, C++, Java, Dart, JavaScript, Rust (Beginner) | Specialized Software : SOLIDWORKS (Beginner), Wireshark (Beginner) | Containerization : Docker, Kubernetes | . Certifications and Awards . Google Inc. : GDSC Lead Tenure Completion | IET MPSTME Hack &amp; Code : Runner-up (2nd Prize) with Aaryadev Chandra, Dev Chandan and Shireen Chand | Udacity : AWS Machine Learning Foundations | LinkedIn : Advanced Linux - The Linux Kernel | LinkedIn : PyTorch Essential Training - Deep Learning | LinkedIn : Tensorflow - Neural Networks and Working with Tables | LinkedIn : Unix Essential Training | Coursera : Building Modern Python Applications on AWS | Kaggle : Intermediate Machine Learning | Kaggle : Intro to Machine Learning | Kaggle : Intro to AI Ethics | Kaggle : Pandas | Kaggle : Python | Stepik : Data Structures | Coursera : Programming for Everybody(Getting Started with Python) | Goldman Sachs : Engineering Virtual Program | . Open Source Contributions . I have only started out in contributing to Open source repositories, but I am trying my best to be able to work harder and make sure I can give back to the community! . FaceX #952 - Finding convenient way to reduce time consumptions on EfficientNets | FaceX #949 - Implementation of an InceptionResnet-v2 model for Face Recognition | TheUpdateFramework #1574 - Ngclient: persist metadata safely 1. | . My first open-source contribution. Made a lot of mistakes, but thanks to the patience and guidance of @jku, I was able to finally make the contribution! &#8617; . |",
          "url": "https://suvadityamuk.github.io/portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://suvadityamuk.github.io/portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}